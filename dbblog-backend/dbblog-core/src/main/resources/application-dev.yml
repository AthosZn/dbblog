spring:
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driverClassName: com.mysql.cj.jdbc.Driver
    druid:
      url: jdbc:mysql://localhost:3306/dbblog?allowMultiQueries=true&useUnicode=true&characterEncoding=UTF-8&useSSL=false&serverTimezone=GMT
      username: root
      password: root
      initial-size: 10
      max-active: 100
      min-idle: 10
      max-wait: 60000
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
      time-between-eviction-runs-millis: 60000
      min-evictable-idle-time-millis: 300000
      #validation-query: SELECT 1 FROM DUAL
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false
      stat-view-servlet:
        enabled: true
        url-pattern: /druid/*
        #login-username: admin
        #login-password: admin
      filter:
        stat:
          log-slow-sql: true
          slow-sql-millis: 1000
          merge-sql: false
        wall:
          config:
            multi-statement-allow: true
  # redis 配置
  redis:
    host: localhost
    port: 6379
    timeout: 6s #连接池超时时长
    lettuce:
      pool:
        max-active: 1000 #连接池最大连接数
        max-wait: -1ms #连接池最大阻塞等待时间
        max-idle: 10 #连接池最大空闲连接
        min-idle: 5 #连接池最小空闲连接
  # elasticsearch配置
  data:
    elasticsearch:
      cluster-name: elasticsearch
      cluster-nodes: 127.0.0.1:9300
  # rabbitMQ配置

  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      # 配置消费者消息offset是否自动重置(消费者重连会能够接收最开始的消息)
      auto-offset-reset: earliest
      enable-auto-commit: false
    producer:
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      # 写入失败时，重试次数。当leader节点失效，一个repli节点会替代成为leader节点，此时可能出现写入失败，
      # 当retris为0时，produce不会重复。retirs重发，此时repli节点完全成为leader节点，不会产生消息丢失。
      retries: 3  #  重试次数
        #procedure要求leader在考虑完成请求之前收到的确认数，用于控制发送记录在服务端的持久化，其值可以为如下：
        #acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为-1。
        #acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
      #acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1的设置。
      #可以设置的值为：all, -1, 0, 1
      acks: all
      ack-mode: manual
    listener:
      # 在侦听器容器中运行的线程数。
      concurrency: 5
#      RECORD
#      每处理一条commit一次
#      BATCH(默认)
#      每次poll的时候批量提交一次，频率取决于每次poll的调用频率
#      ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
#        records.forEach(record -> {
#        System.out.printf("topic = %s ,partition = %d,offset = %d, key = %s, value = %s%n", record.topic(), record.partition(),
#        record.offset(), record.key(), record.value());
#      });

#      TIME
#      每次间隔ackTime的时间去commit(跟auto commit interval有什么区别呢？)
#      COUNT
#      累积达到ackCount次的ack去commit
#      COUNT_TIME
#      ackTime或ackCount哪个条件先满足，就commit
#      MANUAL
#      listener负责ack，但是背后也是批量上去
#      MANUAL_IMMEDIATE
#      listner负责ack，每调用一次，就立即commit
      ack-mode: manual_immediate

#  rabbitmq:
#    host: 127.0.0.1
#    port: 5672
#    username: guest
#    password: guest
#mybatis-plus
mybatis-plus:
  global-config:
    #刷新Mapper，只在开发环境打开
    refresh: true
oss:
  qiniu:
    domain: http://oss.dblearn.cn
    prefix: dbblog
    accessKey: ENC(pVGOa/io1AnqiEKkDTVORO3W8nzn+vbOl53TeRBIGgCuWvqUEzOAdIBLmdpoYVmsDHESGVc2QzY=)
    secretKey: ENC(UXM3TEuaIuLrPdKuDASHoZExQyOrgGl7PqV/iB0EbHOFhnUSF3bMol8D2qh+QlthccS5QD0fHbM=)
    bucketName: test

jasypt:
  encryptor:
    password: dbblog-jasypt
